# Data Mining

## The Ancient Art of the Numerati (Data Mining) [http://guidetodatamining.com/](http://guidetodatamining.com/)

### User-based recomendation system

**1. Создаем систему рекомендаций, основанную на оценках пользователей**

Допустим, есть таблица с пользователями, песнями и их оценками (от 1 до 5):

|   | Unforgiven | Paradise | Ghost | Sniper |
| --- | --- | --- | --- | --- |
| Ben | 1.5 |   |   | 4 |
| Tom | 3 | 5 |   |   |
| Ron | 3.5 |   | 2 | 4.5 |
| Sam |   | 3 | 5 |   |

Алгоритмы:

_**1. Minkowski Distance Metric**_

 ![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/Minkowski.png)

  - Euclidean Distance when r=2

  - Manhattan Distance when r=1

```csharp
public static double MinkowskiDistance(User target, User neighbor, double r)

{
    var combinedBySongs = target.Songs.Join(neighbor.Songs,
                                            s => s.Name, s => s.Name,
                                            (t, n) => new { TargetRating = t.Rating, NeighborRating = n.Rating });

    var sum = combinedBySongs.Sum(p => Math.Pow(Math.Abs(p.TargetRating - p.NeighborRating), r));

    returnMath.Pow(sum, 1/r);
}
```

_**2. Cosine Similarity**_

 ![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/Cosine.png) где ||x|| это ![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/Cosine_part2.png)

_**3. Pearson Correlation Coefficient**_

 ![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/PearsonCorrelation.png)

Результат от -1 до 1, где 1 прямая корреляция и схожесть

Реализованный алгоритм ниже:

```csharp
        public static double PearsonCorrelation(User target, User neighbor)
        {
            var combined = target.Songs.Join(neighbor.Songs,
                                             s => s.Name, s => s.Name,
                                             (t, n) => new
                                                      {
                                                         TargetRating = t.Rating,
                                                         NeighborRating = n.Rating
                                                      })
                                       .ToList();
 
            // calculating numerator

            var sumOfProduct = combined.Sum(pair => pair.TargetRating\*pair.NeighborRating);

            var productOfSums = combined.Sum(pair => pair.TargetRating)\*

                                combined.Sum(pair => pair.NeighborRating);

            var numerator = sumOfProduct - (productOfSums / combined.Count);

            // calculating denominator

            var squareOfSum1 = Math.Pow(combined.Sum(pair => pair.TargetRating), 2);

            var sumOfSquare1 = combined.Sum(pair => pair.TargetRating\*pair.TargetRating);

            var sqaureRoot1 = Math.Sqrt(sumOfSquare1 - squareOfSum1/combined.Count);

            var squareOfSum2 = Math.Pow(combined.Sum(pair => pair.NeighborRating), 2);

            var sumOfSquare2 = combined.Sum(pair => pair.NeighborRating \* pair.NeighborRating);

            var sqaureRoot2 = Math.Sqrt(sumOfSquare2 - squareOfSum2 / combined.Count);

            var denominator = sqaureRoot1\*sqaureRoot2;

            return numerator/denominator;

        }
```

_**4. K-nearest neighbor**_

Этот алгоритм позволяет определять рекомендации на основании k ближайших соседей. Сначала считаем соседей и сортируем их по близости (можно использовать для подсчета любой из алгоритмов выше). Затем берем k первых соседей и рассчитываем проценты влияния каждого из соседей. Напр. k=3, **коэффициенты влияния (в данном лсучае расчитанные Pearson Correlation Coefficient)** каждого **0.8 + 0.7 + 0.5 = 2.0** , тогда проценты 40%+35%+25% (отсчет идет от 2, т.е. 0.8/2 = 40%). Затем берем атрибуты каждого из пользователей (в данном случае все отмеченные ими песни) и высчитываем рейтинг каждой из них с использованием процента влияния:

Projected rating = (4.5 x 0.25) + (5 x 0.35) + (3.5 x 0.4)  = 4.27.

После этого сортируем и показываем как рекомендацию n посчитанных rating

**Использование:**

- если вх. данные сплошные (нет или мало пустых ячеек без цифр), то Minkowski

- если вх. данные имеют много пустых ячеек, то лучше Cosine Similarity

- если данные подвержены инфляции (grade-inflation), то лучше Pearson Correlation Coefficient. Инфляция это когда Sam ставит оценки от 1 до 5, а Ben только 4 и 5, то есть 4 Бена может быть равна 2 или 3 Сэма. Чтобы это учесть лучше использовать Pearson Correlation.

_**5. Adjusted Cosine Similarity**_

Формула вляется **Item-based** collaborative filtering recommendation algorithm. Алгоритм учитывает инфляцию пользовательских оценок.

![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/AdjustedCosine.PNG)

![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/AdjustedCosine_part.PNG)

означает рейтинг _R_, который пользователь _u_ дал item'у _i_ минус среднее значение всех рейтингов, которые пользователь когда-либо выставлял (учет инфляции). Это дает нам нормализированный рейтинг. 

В формуле выше для _s(i,j)_ мы находим _схожесть (similarity)_ между item'ами _i_ и _j_. Числитель говорит, что для каждого пользователя, который поставил оценку **обоим** item'ам мы перемножаем нормализованные рейтинги этого пользователя для item'ов _i_ и _j_ и суммируем полученные произведения. В знаменателе мы сначала берем квадраты из всех нормализованных рейтингов для item'a _i_ , суммируем их и берем квадратный корень из результата. Затем делаем тоже самое для item'a _j_. Затем перемножаем получившиеся результаты.

Просчитав каждый item с каждым, получим **матрицу схожести (similarity matrix)**.

**ВЫВОД**

Основным недостатком всех этих алгоритмов является то, что они все не масштабируемы. Если есть 1000000 пользователей и 1000000 песен, то необходимо будет обработать всех пользователей, чтобы найти ближайшего соседа. Держать всех в памяти не реально и вычислять долго. Кроме того, будет много пробелов, т.к. каждый пользователь не мог оценить миллион песен. К тому же пользователи ленивы, не все ставят оценки, либо не обновляют уже проставленные, если речь, например, идет об оценках купленных товаров и пользователь разочаровался в товаре, но он уже не ухудшит оценку. А также пользователи могут лгать в своих оценках.

### Classification based on item attributes

Как уже было сказано выше, системы основанные на пользовательских оценках плохо масштабируются и имеют слишком разряженные матрицы схожести. Более того, такие рекомендации **всегда** будут рекомендовать популярные элементы (песни, фильмы и т.д.) и игнорировать непопулярные или прсто новые, которые еще никто не оценил. Поэтому лучше использовать **классификацию по атрибутам каждого элемента**, чтобы найти похожие и опустить пользовательские субъективные оценки.

Предположим, нам необходимо оценить схожесть между песнями для рекомендации их пользователю. Введем несколько атрибутов для каждой песни, например, звучание пианино, вокал, бэк-вокал, гитарные рифы. Каждый из этих атрибутов оценим по шкале от 1 до 5, чем больше число, тем больше присутствеи (влияние) данного атрибута в песне.

|   | Piano | Vocal | Backup Vocal | Guitar riffs |
| --- | --- | --- | --- | --- |
| Dr. Dog/Fate | 2.5 | 4  | 4  | 5 |
| Phoenix/Lisztomania | 2 | 5 | 1 | 2 |
| Heartless Bastards/Out at Sea | 1 | 5 | 1 | 4 |
| Todd Snider/Don't Tempt Me | 4 | 5 | 5 | 1 |

Как видно, изначальная матрица уже **не разряжена**.
Теперь можно расчитать _**The Manhattan Distance**_ между Dr. Dog/Fate и Phoenix/Lisztomania для каждого атрибута:

|   | Piano | Vocal | Backup Vocal | Guitar riffs |
| --- | --- | --- | --- | --- |
| Dr. Dog/Fate | 2.5 | 4  | 4  | 5 |
| Phoenix/Lisztomania | 2 | 5 | 1 | 2 |
| **Distance** | **0.5** | **1** | **3** | **3** |

Суммируя эти расстояния поулчаем _**The Manhattan Distance**_ равное **7,5**. Таким образом, модно посчитать расстояния между каждого с каждым и иметь представление о схожести основываясь только на атрибутах элементов.

#### Normalization

Допустим, есть необходимость добавить характеристику "удары в минуту" (beats per minute). Тогда начальная таблица будет выглядеть так:

|   | Piano | Vocal | Backup Vocal | Guitar riffs | BPM |
| --- | --- | --- | --- | --- | --- |
| Dr. Dog/Fate | 2.5 | 4  | 4  | 5 | 140 |
| Phoenix/Lisztomania | 2 | 5 | 1 | 2 | 110 |
| Heartless Bastards/Out at Sea | 1 | 5 | 1 | 4 | 130 |
| Todd Snider/Don't Tempt Me | 4 | 5 | 5 | 1 | 88 |

После добавления данная характеристика вносит хаос в наши расчеты расстояния, так как она доминирует над остальными атрибутами (сотни против единиц).

Для того, чтобы избавиться от такой ситуации нужно использовать **Нормализацию** оценки. Нормализация позволяет стндартизировать оценку. 
Ее результатом является число от 0 до 1 для каждого атрибута.

_**1. Простая (грубая) нормализация**_

 Максимум BPM = 140, минимум = 88. Диапазон между max и min равен 52. Теперь для подсчета нормализованного значения мы вычитаем из **реального значения** атрибута **минимум** и делим это выражение на **диапазон**.
 Для Phoenix/Lisztomania нормализованное значение будет равно (110 - 88)/52 = 0,423
 
 |   | Piano | Vocal | Backup Vocal | Guitar riffs | BPM | BPM Norm. |
| --- | --- | --- | --- | --- | --- | --- |
| Dr. Dog/Fate | 2.5 | 4  | 4  | 5 | 140 | 1.0 |
| Phoenix/Lisztomania | 2 | 5 | 1 | 2 | 110 | 0,423 |
| Heartless Bastards/Out at Sea | 1 | 5 | 1 | 4 | 130 | 0.808 |
| Todd Snider/Don't Tempt Me | 4 | 5 | 5 | 1 | 88 | 0 |

_**2. The Standard Score (aka z-score)**_

Standard Score показывает насколько велико отклонение (deviation) у конкретного значения (value) от среднего (mean).

![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/Standard_Score.PNG)

**Standard Deviation** считается по формуле:
 
![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/Standard_Deviation.PNG)    
 
 card(x) - это мощность x, то есть количество значений x вообще.
 
 Сумма всех BPM = **468**. Всего их **4**, значеит среднее (**mean**) равно **117**. Таким образом, Standard Deviation будет равна:
 
 sd = sqrt(((140 - 117)^2 + (110 - 117)^2 + (130 - 117)^2 + (88 - 117)^2) / 4) = sqrt((529 + 49 + 169 + 841) / 4) = sqrt(397) = **19.92**

Зная Standard Deviation, можно посчитать Standard Score для Phoenix/Lisztomania например:

_StandardScore(Phoenix/Lisztomania) = (110 - 117) / 19.92 = -0.351_

Проблема в расчетах **Standard Score** в том, что оно подвержено влиянию выходящих за средние рамки элементов. Например, есть 100 рабочих зарабатывающих 10$/час и есть CEO, который имеет 6 млн. в год, средняя почасовая заработная плата будет:

( 100 * $10 + 6,000,000 / (40 * 52)) / 101 = (1000 + 2885) / 101 = $38/hr. 

Таким образом влияние заработка CEO слишком велико даже для 100 рабочих. Для нивелирования этого эффекта можно воспользоваться следующей нармализацией.

_**3. Modified Standard Score**_

Для расчета **Modified Standard Score** необходимо заменить _mean_ **медианой (median)** и заменить _стандартную девиацию* **абсолютной стандартной девиацией (absolute standard deviation)**. 

Такми образом, формула **Modified Standard Score**:

![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/Modified_Standard_Score.PNG)

Чтобы подсчитать **Медиану** нужно расположить значения от низшего к высшему и выбрать среднее из них. Если количество значений четное, то **медиана** - это среднее арифметическое двух значений посредине. 

**Absolute Standard Deviation** считается по формуле:

![](https://github.com/SergeyUsok/SelfEducationNotes/blob/master/img/DataMining/Absolute_Standard_Deviation.PNG)

Вообще нормализация необходима, когда условия ниже соблюдены:
- **необходимо расчитать расстояние между элементами, базируясь на числовых значениях их атрибутов**
- **числовые значения разных атрибутов существенно (на порядок) отличаются и атрибут с большими значениями сильно повлияет на результаат вычисления**. Примером могут выступить атрибуты квартиры (количество комнат, площадь и **цена**, значение которой всегда будет больше относительно первых двух.)

Некоторые различают понятия **нормализация** и **стандартизация**. Для них **нормализация** - это масштабирование значений таки образом, что они находятся в диапазоне от 0 до 1, а **стандартизация** - это взятие за основу того, что медиана равно 0, а все значения - это девиации от 0 в обе стороны. Т.о., для них **Standard Score** и **Modified Standard Score** являются примерами **стандартизации**.

Однако нормализация имеет свою цену в увеличении количества вычислений (нормализировавть нужно все атрибуты, если хотя бы один из них иммет резко отличное значение, например, десятки против сотен) и соответственно просадки производительности.

#### Classification

**Классификатор** - это программа, которая использует атрибуты объекта для определения к какой группе или классу этот объект принадлежит. Программа использует набор объектов с уже определенным классом и, базируясь на этом наборе, определяет класс для новых неклассифицированных объектов.

Обычно dataset рабивается на несколько частей, одни из которых выступают **training datasets**, а другие - **test datasets**. Например, разбив общий dataset на 3 части, их можно использовать следующим образом:

- iteration1: train with **part1** and **part2** => test with **part3**
- iteration2: train with **part1** and **part3** => test with **part2**
- iteration3: train with **part2** and **part3** => test with **part1**

Наиболее часто разбивают на **10** частей и этот метод называется **10-Fold Cross Validation**. Ипользуя этот метод, мы разбиваем **dataset** на **10** случайных частей. На каждой итерации мы испльзуем **9** частей для **training**, и **1** часть для **testing**. Затем повторяем, но испльзуем уже другую часть для **testing**. Т.о. получается **10** итераций.

После всех итераций получается, что каждая запись из начального **dataset** побывала в **testing** части и для нее алгоритм провел классификацию. Таким образом, имея количество правильно классифицированных записей, можно подсчитать точность алгоритма в процентах:

**accuracy (%) = correctly_classified_records / total_number_of_records**

**10-Fold Cross Validation** позволяет использовать 90% данных из **dataset** на каждой итерации. Это с одной стороны дает неплохую, с другой стороны не всегда достаточную точность. 

**N-Fold Cross Validation** или **Leave-One-Out** - метод валидации ML алгоритма, который позволяет использовать максимум данных из **dataset** на каждой итерации. Имея **dataset** с 1000 записей, мы тренируем классификатор на 999 и тестируем на 1. И делаем 1000 итераций.
Помимо большей точности (high accuracy), **Leave-One-Out** также **детерминирован**, что безусловно является достоинством по сравнению с **10-Fold Cross Validation**. При измерении точности алгоритма с помощью **10-Fold Cross Validation** присутствует фактор случайности, т.к. общий **dataset** разбивается на 10 частей случайным образом и измерения одного и того же алгоритма на одном и том же **dataset**, но в разное время или разными людьми даст немного разные результаты. В то же время **Leave-One-Out** всегда будет давать одни и те же значения.

Недостатком **Leave-One-Out** является большие затраты на вычисления. Например, если классификатору дать **dataset** из 1000 записей и ему необходимо 1 минуту для тренировки, то в случае **10-Fold Cross Validation** мы потратим 10 минут на все тренировки. В случае **Leave-One-Out** нам понадобится 16 часов. А если **dataset** будет сотосять из 1 000 000 записей?

Вторым недостатком является **Stratification**. Поскольку входящий **dataset** может содержать классифицируемые категории неравномерно, то в случае **10-Fold Cross Validation** мы разбиваем **dataset** на части и размещаем в каждой части все категории пропорцианально тому, в каких пропорциях они присутствуют в изначальном **dataset**. В случае с **Leave-One-Out** мы всегда испоользуем весь **dataset** разом, кроме одной записи, для тренировки. Это может привести к некорректному результату обучения алгоритма. Это и есть **Stratification** (можно перевести как наслоение). Поэтому **Leave-One-Out** подходит для небольших **dataset**, а на больших **dataset** все еще предпочтительней использовать **10-Fold Cross Validation**.

**Kappa Statistic** - позволяет измерить насколько точность (**accuracy**) классификации плоха, хороша или отлична. Для этого используется сравнение точности классфикации нашего классфикатора с точностью **random** классификатора, основанного просто на пропорциях присутствия каждой категории в изначального **dataset**. Подробнее в интернете.


